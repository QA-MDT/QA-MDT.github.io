<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
  <title>Summary</title>
  <style>
    body {
      font-family: 'Roboto', sans-serif;
      background: url('5de0c68292a6c1575011970542.jpg') no-repeat center center fixed; /* 设置背景图 */
      background-size: cover; /* 使背景图覆盖整个页面 */
      margin: 0;
      padding: 0;
    }
    .container {
      width: 80%;
      margin: auto;
      background-color: rgba(255, 255, 255, 0.9); /* 浅灰色带透明度 */
      padding: 20px;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    }
    h1 {
      color: #2166ac;
      text-align: center;
      font-size: 2.2em;
      margin-bottom: 0.5em;
    }
    h2 {
      color: #4d4d4d; /* 深灰色 */
      text-align: center;
      margin-top: 2em;
    }
    p {
      color: #4d4d4d; /* 深灰色文字 */
      text-align: center;
      font-size: 1.5em; /* 放大文字 */
      margin-top: 0.5em;
      margin-bottom: 0.5em;
      padding-left: 12%; /* 根据需要调整百分比 */
      padding-right: 12%; /* 根据需要调整百分比 */
    }
    .abstract-text {
      margin-left: 20px;
      margin-right: 20px;
      text-align: justify;
    }
    .pdf-image {
      display: block;
      margin: 40px auto;
      width: 73%; /* 根据需要调整宽度 */
    }
  </style>
</head>

<body>
  <div class="container">
    <header role="banner">
      <h1>Quality-aware Masked Diffusion Transformer for</h1>
      <h1>Enhanced Music Generation</h1>
      <p>Anonymous<sup>*1</sup></p>
      <p><sup>1</sup>Anonymous University</p>
    </header>
    <section>
      <h2 id="abstract">
        <font color="000000">Abstract</font>
      </h2>
      <p class="abstract-text">
        <font color="000000"> In recent years, diffusion-based text-to-music (TTM) generation has gained prominence, offering a novel approach to synthesizing musical content from textual
          descriptions. Achieving high accuracy and diversity in this generation process
          requires extensive, high-quality data, which often constitutes only a fraction of
          available datasets. Within open-source datasets, the prevalence of issues like
          mislabeling, weak labeling, unlabeled data, and low-quality music waveform significantly hampers the development of music generation models. To overcome
          these challenges, we introduce a novel quality-aware masked diffusion transformer
          (QA-MDT) approach that enables generative models to discern the quality of input
          music waveform during training. Building on the unique properties of musical
          signals, we have adapted and implemented a MDT model for TTM task, while
          further unveiling its distinct capacity for quality control. Moreover, we address the
          issue of low-quality captions with a caption refinement data processing approach.</font>
      </p>
      <img src="0520_music-第 1 页.png" alt="PDF Image" class="pdf-image">
      <img src="截屏2024-05-26 18.31.21.png" alt="PDF Image" class="pdf-image">
    </section>
  </div>
</body>

</html>
